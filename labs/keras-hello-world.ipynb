{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the MNIST dataset using Keras's load_data method\n",
    "Since class labels need to be organised into one-hot-encoded vectors we do that using keras.utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# convert class labels to binary class one-hot-encoded vectors\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float before normalising\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalise by dividing by (max-min) i.e. (255-0)\n",
    "X_train/=255\n",
    "X_test/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data (60000, 28, 28)\n",
      "Example of a y label convereted to ohe: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the training data', X_train.shape)\n",
    "print('Example of a y label convereted to ohe:', y_train_ohe[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST ANN\n",
    "We will now use Keras to build a basic fully connected ANN. \n",
    "We create a sequential model\n",
    "\n",
    "Firstly the input shape which as 2-dimensional representation of the pixels i.e. 28x28 needs to be flattened.\n",
    "We achieve this by adding a Flatten layer.\n",
    "Thereafeter we add desnse layers followed by activation layers. \n",
    "Apart from the input first Desnse layer ; all other layers need only specify the number of neurons / hidden units to be included in a given hidden layer. That number automatically becomes input to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add (Flatten( input_shape=(28, 28)))\n",
    "model.add(Reshape([28,28,1]))\n",
    "\n",
    "model.add (Dense ( input_dim=28*28, units = 512))\n",
    "\n",
    "model.add ( Activation('sigmoid'))\n",
    "\n",
    "model.add (Dense ( units = 128))\n",
    "\n",
    "model.add ( Activation('sigmoid'))\n",
    "\n",
    "model.add (Dense ( units = 10))\n",
    "\n",
    "model.add ( Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_18 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the optimiser for the network wight update strategy and then compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass optimizer by name: default parameters will be used\n",
    "opt = optimizers.SGD(lr=0.01)\n",
    "#specify the loss function and the metric for evaluation\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 1152/60000 [..............................] - ETA: 9s - loss: 0.0896 - acc: 0.1302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0896 - acc: 0.1283\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0896 - acc: 0.1285\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0896 - acc: 0.1328\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0895 - acc: 0.1372\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0895 - acc: 0.1383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d338155a20>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_ohe, batch_size=128, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 98us/step\n",
      "Total loss on training 0.0896342469215393\n",
      "Total accuracy on training 0.128\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test_ohe)\n",
    "print('Total loss on training', score[0])\n",
    "print('Total accuracy on training', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "Consider how you might improve the accuracy of the basic neural network for MNIST.\n",
    "- try a different loss functions such as categorical_crossentropy (see https://keras.io/losses/#available-loss-functions)\n",
    "which is known to work better with the softmax layer than does the mean squared error loss. The cross-entropy loss calculates the error rate between the predicted value and the original value. The formula for calculating cross-entropy loss is given here https://en.wikipedia.org/wiki/Cross_entropy. Categorical is used because there are 10 classes to predict from. If there were 2 classes, we would have used binary_crossentropy.\n",
    "- try different optimizers such as Adam and RMSprop (see https://keras.io/optimizers/). For instance the Adam optimizer is an improvement over SGD(Stochastic Gradient Descent). The optimizer is responsible for updating the weights of the neurons via backpropagation. It calculates the derivative of the loss function with respect to each weight and subtracts it from the weight. \n",
    "- try different activation functions such as relu or tanh (see https://keras.io/activations/#available-activations).\n",
    "- try increasing the number of hidden units. However the more complex the network the longer it takes for training. \n",
    "- try adding hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for MNIST\n",
    "Next we look at how to use the CNN implmentation of Keras on the MNIST data. \n",
    "\n",
    "Here we need to reshape our data such that we maintain the 2-dimensional 28x28 representation; instead of having to  flattening it as we did for the basic ANN above. Because we are using only a grey scale representation we will need to also set the number of channels as 1 (instead of say 3 in case we used a RGB input). \n",
    "\n",
    "Keras allows us to specify the number of filters we want and the size of the filters. So, in our first layer, 32 is number of filters and (3, 3) is the size of the filter. We also need to specify the shape of the input which is (28, 28, 1), but we have to specify it only once.\n",
    "\n",
    "The second layer is the Activation layer. We have used ReLU (rectified linear unit) as our activation function. ReLU function is f(x) = max(0, x), where x is the input. It sets all negative values in the matrix ‘x’ to 0 and keeps all the other values constant. It is the most used activation function since it reduces training time and prevents the problem of vanishing gradients.\n",
    "\n",
    "The third layer is the MaxPooling layer. MaxPooling layer is used to down-sample the input to enable the model to make assumptions about the features so as to reduce over-fitting. It also reduces the number of parameters to learn, reducing the training time.\n",
    "\n",
    "It’s a best practice to always do BatchNormalization. BatchNormalization normalizes the matrix after it is been through a convolution layer so that the scale of each dimension remains the same. It reduces the training time significantly.\n",
    "\n",
    "After creating all the convolutional layers, we need to flatten them, so that they can act as an input to the Dense layers.\n",
    "\n",
    "Dense layers are keras’s alias for Fully connected layers. These layers give the ability to classify the features learned by the CNN.\n",
    "\n",
    "Dropout is the method used to reduce overfitting. It forces the model to learn multiple independent representations of the same data by randomly disabling neurons in the learning phase. In our model, dropout will randomnly disable 20% of the neurons.\n",
    "\n",
    "The second last layer is the Dense layer with 10 neurons. The neurons in this layer should be equal to the number of classes we want to predict as this is the output layer.\n",
    "\n",
    "The last layer is the Softmax Activation layer. Softmax activation enables us to calculate the output based on the probabilities. Each class is assigned a probability and the class with the maximum probability is the model’s output for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n",
      "X_test original shape (10000, 28, 28)\n",
      "y_test original shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the data for the CNN\n",
    "Now the shape of X_train is (60000, 28, 28, 1). As all the images are in grayscale, the number of channels is 1. If it was a color image, then the number of channels would be 3 (R, G, B).\n",
    "\n",
    "Here we’ve rescaled the image data so that each pixel lies in the interval [0, 1] instead of [0, 255]. It is always a good idea to normalize the input so that each dimension has approximately the same scale.\n",
    "\n",
    "Now, we need to one-hot encode the labels i.e. Y_train and Y_test. In one-hot encoding an integer is converted to an array which contains only one ‘1’ and the rest elements are ‘0’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.core import Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three steps to create a CNN\n",
    "# 1. Convolution\n",
    "# 2. Activation\n",
    "# 3. Pooling\n",
    "# Repeat Steps 1,2,3 for adding more hidden layers\n",
    "\n",
    "# 4. After that make a fully connected network\n",
    "# This fully connected network gives ability to the CNN\n",
    "# to classify the samples\n",
    "\n",
    "model_cnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cnn.add(Reshape([28,28,1]))\n",
    "\n",
    "model_cnn.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "model_cnn.add(BatchNormalization(axis=-1))\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(Conv2D(32, (3, 3)))\n",
    "model_cnn.add(BatchNormalization(axis=-1))\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Conv2D(64,(3, 3)))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_cnn.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model_cnn.add(Dense(128))\n",
    "model_cnn.add(BatchNormalization())\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(Dropout(0.2))\n",
    "model_cnn.add(Dense(10))\n",
    "\n",
    "model_cnn.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 601,578\n",
      "Trainable params: 601,194\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = optimizers.Adam(lr=0.01)\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce over-fitting, we use another technique known as Data Augmentation. Data augmentation rotates, shears, zooms, etc the image so that the model learns to generalize and not remember specific data. If the model overfits, it will perform very well on the images that it already knows but will fail if new images are given to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we can do Data Augmentation in Keras. You can play with the values and check if it improves the accuracy of the model.\n",
    "\n",
    "We have to create batches, so that we use less memory. Moreover, we can also train our model faster by creating batches. Here we are using batch of 64, so the model will take 64 images at a time and train on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit the model using one epoch.\n",
    "You will notice that this does take quite a long time compared to the ANN. \n",
    "This is because there are many more parameters in a CNN to work with ; \n",
    "however there is a significant improvement in accuracy.\n",
    "We have to create batches, so that we use less memory. Moreover, we can also train our model faster by creating batches. \n",
    "Here we are using batch of 64, so the model will take 64 images at a time and train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 662s 11ms/step - loss: 0.1142 - acc: 0.9642 - val_loss: 0.0609 - val_acc: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d3369fc828>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(X_train, y_train_ohe, batch_size=64, nb_epoch=1, validation_data=(X_test, y_test_ohe))# model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 27s 3ms/step\n",
      "\n",
      "Test accuracy:  0.9812\n"
     ]
    }
   ],
   "source": [
    "score = model_cnn.evaluate(X_test, y_test_ohe)\n",
    "print()\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the actual and predicted labels for the test set into a file...\n"
     ]
    }
   ],
   "source": [
    "print('Saving the actual and predicted labels for the test set into a file...')\n",
    "predictions  = model_cnn.predict_classes(X_test)\n",
    "\n",
    "predictions = list(predictions)\n",
    "actuals = list(y_test)\n",
    "\n",
    "sub = pd.DataFrame({'Actual': actuals, 'Predictions': predictions})\n",
    "sub.to_csv('output_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
